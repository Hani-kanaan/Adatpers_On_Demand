{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "name": "adapters_on_demand",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 13971973,
          "sourceType": "datasetVersion",
          "datasetId": 8907363
        },
        {
          "sourceId": 14731370,
          "sourceType": "datasetVersion",
          "datasetId": 9413507
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hani-kanaan/Adatpers_On_Demand/blob/main/adapters_on_demand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "7BZWa5Tumx3X"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "hanikanaan774_google_cluster_sample_path = kagglehub.dataset_download('hanikanaan774/google-cluster-sample')\n",
        "hanikanaan774_prompts_path = kagglehub.dataset_download('hanikanaan774/prompts')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ryx33nAYmx3b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install -U transformers peft accelerate\n",
        "!pip install -U accelerate peft\n",
        "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install transformers peft accelerate datasets\n",
        "!pip install safetensors\n"
      ],
      "metadata": {
        "id": "h_fxISvnUB0y",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall sentence-transformers -y"
      ],
      "metadata": {
        "trusted": true,
        "id": "PhiZ2O-Bmx3h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-05T08:07:57.454956Z",
          "iopub.execute_input": "2026-02-05T08:07:57.455397Z",
          "iopub.status.idle": "2026-02-05T08:09:19.701622Z",
          "shell.execute_reply.started": "2026-02-05T08:07:57.455372Z",
          "shell.execute_reply": "2026-02-05T08:09:19.700219Z"
        },
        "id": "hR8iHLKumx3j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer , BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "#linux quantization:\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    # quantization_config=bnb_cfg,\n",
        "    #torch_dtype=torch.float16,\n",
        "    quantization_config= bnb_cfg ,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "QuNOtMZIVHGi",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#adapter from: https://huggingface.co/DreamGallery/Qwen-Qwen2.5-1.5B-Instruct-1727452927\n",
        "# from peft import PeftModel\n",
        "\n",
        "# model = PeftModel.from_pretrained(\n",
        "#     base_model,\n",
        "#     \"DreamGallery/Qwen-Qwen2.5-1.5B-Instruct-1727452927\",  # adapter path\n",
        "#     trust_remote_code=True\n",
        "# )\n"
      ],
      "metadata": {
        "id": "wj8uzRPLaafr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain the difference between L1 and L2 regularization in machine learning.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
        "with torch.no_grad():\n",
        "    out = base_model.generate(**inputs, max_new_tokens=64)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "Vt2c5ZIj4Fto",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset of prompts:\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds1 = load_dataset(\"fka/awesome-chatgpt-prompts\")"
      ],
      "metadata": {
        "id": "yD3IOmZ319QX",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"RUC-DataLab/DataScience-Instruct-500K\")"
      ],
      "metadata": {
        "id": "rT-jRbNnwAa_",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds2 = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "wfMAJfaHmx32"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "print(datasets.__version__)"
      ],
      "metadata": {
        "trusted": true,
        "id": "inyU1j4Dmx34"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_ds(batch):\n",
        "    prompts = []\n",
        "    targets = []\n",
        "\n",
        "    for messages, evaluation in zip(batch[\"messages\"], batch[\"evaluation\"]):\n",
        "        if isinstance(messages, list):\n",
        "            prompt = \"\\n\".join([f\"{m.get('role', '')}: {m.get('content', '')}\" for m in messages])\n",
        "        else:\n",
        "            prompt = str(messages)\n",
        "\n",
        "        prompts.append(prompt)\n",
        "        targets.append(str(evaluation))\n",
        "\n",
        "    return tokenizer(\n",
        "        prompts,\n",
        "        text_target=targets,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "def tokenize_ds1(batch):\n",
        "    prompts = []\n",
        "    targets = []\n",
        "\n",
        "    for act, prompt in zip(batch[\"act\"], batch[\"prompt\"]):\n",
        "        inst = \" \".join(act) if isinstance(act, list) else str(act)\n",
        "        prompts.append(inst)\n",
        "        targets.append(str(prompt))\n",
        "\n",
        "    return tokenizer(\n",
        "        prompts,\n",
        "        text_target=targets,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "def tokenize_ds3(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"Question\"],\n",
        "        text_target=batch[\"Response\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "train1 = ds.map(tokenize_ds, batched=True, batch_size=256)\n",
        "train2 = ds1.map(tokenize_ds1, batched=True, batch_size=256)\n",
        "\n"
      ],
      "metadata": {
        "id": "nj4Q8XT01TrE",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train3 = ds2.map(tokenize_ds3, batched=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vlb350FNmx36"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "lora_cfg_1 = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "lora_cfg_2 = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "VyOlFkmymx37"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Current device:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "trusted": true,
        "id": "I4bvzIhwmx37"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "id": "4YBwPxDmmx38"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model\n",
        "from transformers import TrainingArguments, Trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model1 = get_peft_model(base_model, lora_cfg_1)\n",
        "args1 = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    output_dir=\"./adapter1\",\n",
        "    gradient_accumulation_steps=1,\n",
        "    logging_dir=\"./logs\",         # where logs are saved\n",
        "    logging_steps=5,              # print every 5 steps\n",
        "    report_to=\"none\",             # disable WandB, keep console output\n",
        "    disable_tqdm=False            # show progress bar\n",
        ")\n",
        "small_train1 = train1[\"train\"].select(range(100))  # use only first 100 samples for faster runtime\n",
        "\n",
        "trainer1 = Trainer(model=model1, args=args1, train_dataset=small_train1)\n",
        "trainer1.train()\n",
        "model1.save_pretrained(\"./adapters/adapter1\")\n",
        "\n",
        "del model1\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model2 = get_peft_model(base_model, lora_cfg_2)\n",
        "args2 = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    output_dir=\"./adapter2\",\n",
        "    gradient_accumulation_steps=1,\n",
        "    logging_dir=\"./logs\",         # where logs are saved\n",
        "    logging_steps=5,              # print every 5 steps\n",
        "    report_to=\"none\",             # disable WandB, keep console output\n",
        "    disable_tqdm=False            # show progress bar\n",
        ")\n",
        "small_train2 = train2[\"train\"].select(range(100))\n",
        "trainer2 = Trainer(model=model2, args=args2, train_dataset=small_train2)\n",
        "trainer2.train()\n",
        "model2.save_pretrained(\"./adapters/adapter2\")\n",
        "\n",
        "\n",
        "#adapter 3:\n",
        "\n",
        "model3 = get_peft_model(base_model, lora_cfg_2)\n",
        "args3 = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    output_dir=\"./adapter3\",\n",
        "    gradient_accumulation_steps=1,\n",
        "    logging_dir=\"./logs\",         # where logs are saved\n",
        "    logging_steps=5,              # print every 5 steps\n",
        "    report_to=\"none\",             # disable WandB, keep console output\n",
        "    disable_tqdm=False            # show progress bar\n",
        ")\n",
        "small_train3 = train3[\"train\"].select(range(100))\n",
        "trainer3 = Trainer(model=model3, args=args3, train_dataset=small_train3)\n",
        "trainer3.train()\n",
        "model3.save_pretrained(\"./adapters/adapter3\")\n"
      ],
      "metadata": {
        "id": "eBeere3OdFso",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "def get_dir_size_mb(path):\n",
        "    total_size = 0\n",
        "    for root, _, files in os.walk(path):\n",
        "        for f in files:\n",
        "            fp = os.path.join(root, f)\n",
        "            if os.path.isfile(fp):\n",
        "                total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 * 1024)  # MB\n",
        "\n",
        "def get_adapter_vram_mb(adapter_path, dtype_bytes=2):\n",
        "    \"\"\"\n",
        "    dtype_bytes:\n",
        "        fp16 / bf16 = 2\n",
        "        fp32 = 4\n",
        "    \"\"\"\n",
        "    adapter_file = os.path.join(adapter_path, \"adapter_model.safetensors\")\n",
        "    state_dict = load_file(adapter_file)  # SAFE loader\n",
        "    total_params = sum(t.numel() for t in state_dict.values())\n",
        "    vram_mb = total_params * dtype_bytes / (1024 * 1024)\n",
        "    return vram_mb\n",
        "\n",
        "# Configuration\n",
        "base_path = \"/kaggle/working/adapters\"\n",
        "adapter_names = [\"adapter1\", \"adapter2\", \"adapter3\"]\n",
        "\n",
        "# Process each adapter\n",
        "print(\"=\" * 55)\n",
        "print(\"ADAPTER SIZE ANALYSIS\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "for adapter_name in adapter_names:\n",
        "    adapter_path = os.path.join(base_path, adapter_name)\n",
        "\n",
        "    print(f\"\\nðŸ“¦ {adapter_name}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    if os.path.exists(adapter_path):\n",
        "        # Disk size\n",
        "        size_mb = get_dir_size_mb(adapter_path)\n",
        "        print(f\"  ðŸ’¾ Disk size:   {size_mb:.2f} MB\")\n",
        "\n",
        "        # VRAM size\n",
        "        vram_cost = get_adapter_vram_mb(adapter_path, dtype_bytes=2)\n",
        "        print(f\" VRAM cost:   {vram_cost:.2f} MB\")\n",
        "    else:\n",
        "        print(f\" Directory not found: {adapter_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 55)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1QYuWY6Wmx3-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "adapter_path = \"./adapters/adapter1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model.eval()\n",
        "\n",
        "text =\"\"\"### Instruction:\n",
        "explain the difference between L1 and L2 regularization in machine learning.\n",
        "\n",
        "### Input:\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7, top_p=0.9)\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(decoded)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-yDyKuVrmx4A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall transformers sentence-transformers -q\n",
        "\n",
        "# Restart kernel after this\n",
        "import transformers\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "2zSsjMYumx4A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "!pip list | grep -E \"transformers|sentence\"\n",
        "!pip install transformers==4.57.1 sentence-transformers==2.2.2 -q\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-05T10:27:16.132862Z",
          "iopub.execute_input": "2026-02-05T10:27:16.133161Z",
          "iopub.status.idle": "2026-02-05T10:28:51.413782Z",
          "shell.execute_reply.started": "2026-02-05T10:27:16.13313Z",
          "shell.execute_reply": "2026-02-05T10:28:51.41232Z"
        },
        "id": "K0thxQFVmx4B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer , util\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "adapter_tasks = [\n",
        "    \"role-playing instruction following across many creative and utility roles\",\n",
        "    \"data science and machine learning problem solving with step by step reasoning\",\n",
        "    \"medical diagnosis and clinical reasoning with detailed chain-of thought explanations\"\n",
        "]\n",
        "adapter_embs = model.encode(adapter_tasks)\n",
        "\n",
        "print(adapter_embs)\n",
        "def select_adapter(user_prompt):\n",
        "    prompt_emb = model.encode([user_prompt], normalize_embeddings=True)[0]\n",
        "    sims = util.cos_sim(prompt_emb, adapter_embs)[0]\n",
        "    top = sims.argmax().item()\n",
        "    best_score = sims[top].item()\n",
        "    return adapter_tasks[top], best_score\n",
        "task, score = select_adapter(\"Can you summarize this text?\")\n",
        "print(task, score)\n"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2026-02-05T11:23:28.452194Z",
          "iopub.execute_input": "2026-02-05T11:23:28.452455Z",
          "iopub.status.idle": "2026-02-05T11:24:23.920425Z",
          "shell.execute_reply.started": "2026-02-05T11:23:28.452427Z",
          "shell.execute_reply": "2026-02-05T11:24:23.919208Z"
        },
        "id": "x2hDKJIzmx4C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"medical terms prompt\"\n",
        "adapter, scores = select_adapter(prompt)\n",
        "print(adapter)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-05T11:24:47.014615Z",
          "iopub.execute_input": "2026-02-05T11:24:47.014987Z",
          "iopub.status.idle": "2026-02-05T11:24:47.038152Z",
          "shell.execute_reply.started": "2026-02-05T11:24:47.014951Z",
          "shell.execute_reply": "2026-02-05T11:24:47.036742Z"
        },
        "id": "POAfYgIEmx4D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/kaggle/input/google-cluster-sample/borg_traces_data.csv')\n",
        "prompts_df = pd.read_excel('/kaggle/input/prompts/Prompts.xlsx')\n",
        "df[\"collection_id\"] = df[\"collection_id\"].astype(int) % 3\n",
        "df.head()\n",
        "prompts = prompts_df['Prompt'].dropna().tolist()\n",
        "\n",
        "\n",
        "\n",
        "prompts_to_assign = []\n",
        "for i in range(len(df)):\n",
        "    # Cycle through prompts if needed\n",
        "    prompt_idx = i % len(prompts)\n",
        "    prompts_to_assign.append(prompts[prompt_idx])\n",
        "\n",
        "df['collection_type'] = prompts_to_assign\n",
        "df[\"collection_id\"] = df[\"collection_id\"].astype(int) % 3\n",
        "print(f\"\\nUpdated dataset shape: {df.shape}\")\n",
        "print(\"\\nFirst 5 rows with new collection_type:\")\n",
        "print(df[['time', 'collection_id', 'collection_type', 'assigned_memory']].head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "DRNY6ksAmx4E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompts_to_assign)"
      ],
      "metadata": {
        "trusted": true,
        "id": "FpTp76vImx4F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from collections import defaultdict, deque\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import os\n",
        "class AdapterEnv(gym.Env):\n",
        "\n",
        "    def __init__(self, arrivals_by_time, num_adapters=3, num_gpus=2, max_steps=200):\n",
        "        super().__init__()\n",
        "        self.num_adapters = num_adapters\n",
        "        self.num_gpus = num_gpus\n",
        "        self.max_steps = max_steps\n",
        "        self.arrivals_by_time = arrivals_by_time\n",
        "        self.max_time = max(arrivals_by_time.keys()) if arrivals_by_time else 0\n",
        "\n",
        "        # Adjusted adapter sizes - make adapter 2 more competitive\n",
        "        self.adapter_vram = np.array([0.3, 1,1])  # 30%, 40%, 50%\n",
        "\n",
        "        # Action space\n",
        "        self.action_space = gym.spaces.Discrete(4 * num_adapters * num_gpus)\n",
        "\n",
        "        # State space\n",
        "        self.observation_space = gym.spaces.Tuple((\n",
        "            gym.spaces.MultiBinary(num_gpus * num_adapters),\n",
        "            gym.spaces.Box(low=0.0, high=1.0, shape=(num_gpus,)),\n",
        "            gym.spaces.Discrete(100),\n",
        "            gym.spaces.Discrete(num_adapters + 1)\n",
        "        ))\n",
        "\n",
        "        # Track adapter usage\n",
        "        self.adapter_usage_stats = {0: 0, 1: 0, 2: 0}\n",
        "        self.adapter_load_attempts = {0: 0, 1: 0, 2: 0}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.adapter_loaded = np.zeros((self.num_gpus, self.num_adapters), dtype=int)\n",
        "        self.free_vram = np.ones(self.num_gpus)\n",
        "        self.queue = deque()\n",
        "        self.time = 0\n",
        "        self.step_count = 0\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "        self._arrival_step()\n",
        "        return self._get_state(), {}\n",
        "\n",
        "    def _arrival_step(self):\n",
        "\n",
        "        if self.time in self.arrivals_by_time:\n",
        "            self.queue.extend(self.arrivals_by_time[self.time])\n",
        "\n",
        "\n",
        "\n",
        "    def _get_state(self):\n",
        "        next_adapter_id = self.queue[0][\"adapter_id\"] if self.queue else self.num_adapters\n",
        "        return (\n",
        "            tuple(self.adapter_loaded.flatten()),\n",
        "            tuple(self.free_vram),\n",
        "            min(len(self.queue), 99),\n",
        "            next_adapter_id\n",
        "        )\n",
        "\n",
        "    def step(self, action):\n",
        "        # Decode action\n",
        "        action_type = action // (self.num_adapters * self.num_gpus)\n",
        "        rem = action % (self.num_adapters * self.num_gpus)\n",
        "        adapter_id = rem // self.num_gpus\n",
        "        gpu_id = rem % self.num_gpus\n",
        "\n",
        "        reward = -0.01  # Smaller time penalty\n",
        "        processed_task = False\n",
        "\n",
        "        # Track which adapter is being targeted\n",
        "        self.adapter_load_attempts[adapter_id] += 1\n",
        "\n",
        "        # ---- LOAD (type 0) ----\n",
        "        if action_type == 0:\n",
        "            cost = self.adapter_vram[adapter_id]\n",
        "            if self.adapter_loaded[gpu_id, adapter_id] == 0 and self.free_vram[gpu_id] >= cost:\n",
        "                self.adapter_loaded[gpu_id, adapter_id] = 1\n",
        "                self.free_vram[gpu_id] -= cost\n",
        "\n",
        "                if self.queue and self.queue[0][\"adapter_id\"] == adapter_id:\n",
        "                    reward = 0.3\n",
        "                else:\n",
        "                    reward = -0.05   # discourage speculative loads\n",
        "\n",
        "\n",
        "        # ---- EVICT (type 1) ----\n",
        "        elif action_type == 1:\n",
        "            if self.adapter_loaded[gpu_id, adapter_id] == 1:\n",
        "                self.adapter_loaded[gpu_id, adapter_id] = 0\n",
        "                self.free_vram[gpu_id] += self.adapter_vram[adapter_id]\n",
        "\n",
        "                if self.free_vram[gpu_id] < 0.2:\n",
        "                    reward += 0.4   # emergency eviction\n",
        "                else:\n",
        "                    reward -= 0.05  # pointless eviction\n",
        "\n",
        "\n",
        "        # ---- REUSE (type 2) ----\n",
        "        elif action_type == 2:\n",
        "            if self.queue:\n",
        "                task = self.queue[0]\n",
        "                if (task[\"adapter_id\"] == adapter_id and\n",
        "                    self.adapter_loaded[gpu_id, adapter_id] == 1):\n",
        "                    self.queue.popleft()\n",
        "                    processed_task = True\n",
        "                    reward = 1.0\n",
        "                else:\n",
        "                    reward = -1.0\n",
        "\n",
        "        # ---- SEND (type 3) ----\n",
        "        elif action_type == 3:\n",
        "            if self.queue:\n",
        "                task = self.queue[0]\n",
        "                if (task[\"adapter_id\"] == adapter_id and\n",
        "                    self.adapter_loaded[gpu_id, adapter_id] == 1):\n",
        "                    self.queue.popleft()\n",
        "                    processed_task = True\n",
        "\n",
        "                    # ADJUSTED: Higher reward for sending larger adapters\n",
        "                    # base_reward = 0.8\n",
        "                    # size_bonus = 0.4 * self.adapter_vram[adapter_id]\n",
        "                    # reward += base_reward + size_bonus\n",
        "                    reward = 0.8\n",
        "\n",
        "        # Advance simulation\n",
        "        self.time += 1\n",
        "        self._arrival_step()\n",
        "        self.step_count += 1\n",
        "        self.total_reward += reward\n",
        "\n",
        "        terminated = (\n",
        "            self.step_count >= self.max_steps or\n",
        "            (not self.queue and self.time > self.max_time)\n",
        "        )\n",
        "\n",
        "        info = {\n",
        "            'processed': processed_task,\n",
        "            'adapter_stats': self.adapter_usage_stats.copy(),\n",
        "            'load_attempts': self.adapter_load_attempts.copy()\n",
        "        }\n",
        "\n",
        "        return self._get_state(), reward, terminated, False, info\n",
        "\n",
        "\n",
        "def train_q_learning(env, episodes=5000):\n",
        "    \"\"\"Q-learning training loop\"\"\"\n",
        "\n",
        "\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    alpha = 0.1\n",
        "    gamma = 0.95\n",
        "    epsilon = 0.2\n",
        "    epsilon_decay = 0.995\n",
        "\n",
        "    rewards_history = []\n",
        "    epsilon_history = []\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        while True:\n",
        "            if np.random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "\n",
        "            Q[state][action] += alpha * (\n",
        "                reward + gamma * np.max(Q[next_state]) - Q[state][action]\n",
        "            )\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        rewards_history.append(episode_reward)\n",
        "        epsilon_history.append(epsilon)\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "        if ep % 500 == 0:\n",
        "            avg = np.mean(rewards_history[-500:])\n",
        "            print(f\"Ep {ep:4d} | Avg Reward: {avg:6.2f} | Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    return Q, rewards_history , epsilon_history\n",
        "\n",
        "def test_env_heuristic(env, max_steps=200):\n",
        "    \"\"\"Test with greedy heuristic policy \"\"\"\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    A = env.num_adapters * env.num_gpus\n",
        "    G = env.num_gpus\n",
        "\n",
        "    total_reward = 0\n",
        "\n",
        "    print(f\"\\n=== GOOGLE TRACE HEURISTIC TEST ===\")\n",
        "    print(f\"Start: Queue={len(env.queue) if hasattr(env, 'queue') else state[2]} | \"\n",
        "          f\"VRAM: {[f'{v:.2f}' for v in state[1]]}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        action = 0\n",
        "        action_type_name = \"WAIT\"\n",
        "        adapter_id = 0\n",
        "        gpu_id = 0\n",
        "\n",
        "        if env.queue:\n",
        "            next_adapter = env.queue[0]['adapter_id']\n",
        "            loaded = np.array(state[0]).reshape(env.num_gpus, env.num_adapters)\n",
        "\n",
        "            # 1. SEND if possible\n",
        "            sent = False\n",
        "            for g in range(env.num_gpus):\n",
        "                if loaded[g, next_adapter] == 1:\n",
        "                    action = 3 * A + next_adapter * G + g\n",
        "                    action_type_name = \"SEND\"\n",
        "                    adapter_id = next_adapter\n",
        "                    gpu_id = g\n",
        "                    sent = True\n",
        "                    break\n",
        "\n",
        "            # 2. LOAD if needed\n",
        "            if not sent:\n",
        "                cost = env.adapter_vram[next_adapter]\n",
        "                for g in range(env.num_gpus):\n",
        "                    if env.free_vram[g] >= cost:\n",
        "                        action = 0 * A + next_adapter * G + g\n",
        "                        action_type_name = \"LOAD\"\n",
        "                        adapter_id = next_adapter\n",
        "                        gpu_id = g\n",
        "                        sent = True\n",
        "                        break\n",
        "\n",
        "            # 3. EVICT fallback\n",
        "            if not sent:\n",
        "                for g in range(env.num_gpus):\n",
        "                    for a in range(env.num_adapters):\n",
        "                        if loaded[g, a] == 1:\n",
        "                            action = 1 * A + a * G + g\n",
        "                            action_type_name = \"EVICT\"\n",
        "                            adapter_id = a\n",
        "                            gpu_id = g\n",
        "                            sent = True\n",
        "                            break\n",
        "                    if sent:\n",
        "                        break\n",
        "\n",
        "        # Execute action\n",
        "        prev_queue_len = len(env.queue) if hasattr(env, 'queue') else state[2]\n",
        "        prev_vram = state[1]\n",
        "\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Calculate what changed\n",
        "        new_queue_len = len(env.queue) if hasattr(env, 'queue') else state[2]\n",
        "        queue_change = prev_queue_len - new_queue_len\n",
        "\n",
        "        # Log this step\n",
        "        print(f\"Step {step:3d}: {action_type_name} A{adapter_id}â†’G{gpu_id} | \"\n",
        "              f\"Reward: {reward:+.2f} | \"\n",
        "              f\"Queue:{new_queue_len} |\"\n",
        "              f\"VRAM: {[f'{v:.2f}' for v in state[1]]}\")\n",
        "\n",
        "        if done:\n",
        "            print(f\"Episode terminated at step {step}\")\n",
        "            break\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Total reward: {total_reward:.2f}\")\n",
        "    print(f\"Average reward per step: {total_reward/(step+1):.2f}\")\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def load_google_trace(csv_path, max_rows=50000):\n",
        "    \"\"\"\n",
        "    Load Google Cluster Dataset and map collection_id to {0,1,2}\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
        "\n",
        "    # Detect unnamed index column\n",
        "    sample_df = pd.read_csv(csv_path, nrows=1)\n",
        "    has_unnamed = 'Unnamed: 0' in sample_df.columns\n",
        "\n",
        "    print(f\"Loading {csv_path}...\")\n",
        "    df = pd.read_csv(csv_path, index_col=0 if has_unnamed else None)\n",
        "\n",
        "    if max_rows and len(df) > max_rows:\n",
        "        df = df.head(max_rows).copy()\n",
        "        print(f\"Limited to {max_rows} rows\")\n",
        "\n",
        "    # ---- REQUIRED COLUMNS ----\n",
        "    required_cols = ['time', 'collection_id', 'assigned_memory']\n",
        "    missing = [c for c in required_cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"CSV missing required columns: {missing}\")\n",
        "\n",
        "    # ---- MAP collection_id â†’ {0,1,2} ----\n",
        "    if \"collection_id\" in df.columns:\n",
        "        df[\"collection_id\"] = df[\"collection_id\"].astype(int) % 3\n",
        "    else:\n",
        "        df[\"collection_id\"] = df.index.astype(int) % 3\n",
        "\n",
        "    arrivals_by_time = defaultdict(list)\n",
        "\n",
        "    max_mem = df['assigned_memory'].max()\n",
        "    if max_mem == 0:\n",
        "        max_mem = 1.0\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        t = int(row['time'])\n",
        "\n",
        "        adapter_id = int(row['collection_id']) % 3\n",
        "\n",
        "        task = {\n",
        "            'adapter_id': adapter_id,\n",
        "            'vram': row['assigned_memory'] / max_mem,\n",
        "            'collection_id': int(row['collection_id'])\n",
        "        }\n",
        "\n",
        "        arrivals_by_time[t].append(task)\n",
        "\n",
        "\n",
        "    print(f\"âœ“ Loaded {len(df)} tasks across {len(arrivals_by_time)} time steps\")\n",
        "    print(df[['time','collection_type','collection_id','assigned_memory']].head())\n",
        "\n",
        "    return arrivals_by_time\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    CSV_PATH = \"/kaggle/input/google-cluster-sample/borg_traces_data.csv\"\n",
        "\n",
        "    print(\"Loading Google Cluster Dataset...\")\n",
        "    arrivals_by_time = load_google_trace(CSV_PATH, max_rows=5000)\n",
        "    arrivals = load_google_trace(CSV_PATH)\n",
        "    sample_t = next(iter(arrivals))\n",
        "    print(arrivals[sample_t][:5])\n",
        "    env = AdapterEnv(arrivals_by_time, num_adapters=3, num_gpus=2, max_steps=200)\n",
        "\n",
        "    test_env_heuristic(env)\n",
        "    # Train\n",
        "    print(\"\\nStarting Q-learning...\")\n",
        "    Q_table, rewards ,  epsilon_history= train_q_learning(env, episodes=5000)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ahJw3_drmx4G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "from collections import deque\n",
        "\n",
        "def simple_animate_episode(Q, env, max_steps=100):\n",
        "    \"\"\"Create a simple animated visualization of a single episode with reward and domain tracking\"\"\"\n",
        "\n",
        "    # Run episode and collect data\n",
        "    state, _ = env.reset()\n",
        "    states_history = [state]\n",
        "    actions_history = []\n",
        "    rewards_history = []\n",
        "    cumulative_rewards = [0]  # Start with 0\n",
        "\n",
        "    done = False\n",
        "    step = 0\n",
        "\n",
        "    while not done and step < max_steps:\n",
        "        if state in Q:\n",
        "            action = np.argmax(Q[state])\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        actions_history.append(action)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        states_history.append(state)\n",
        "        rewards_history.append(reward)\n",
        "        cumulative_rewards.append(cumulative_rewards[-1] + reward)  # Track cumulative\n",
        "        step += 1\n",
        "\n",
        "    # Create figure with 2x3 grid (6 plots)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "    # 1. VRAM bar chart (top left)\n",
        "    vram_bars = axes[0, 0].bar(range(env.num_gpus), [0] * env.num_gpus,\n",
        "                               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:env.num_gpus])\n",
        "    axes[0, 0].set_title('GPU VRAM Usage')\n",
        "    axes[0, 0].set_xlabel('GPU ID')\n",
        "    axes[0, 0].set_ylabel('Free VRAM')\n",
        "    axes[0, 0].set_ylim(0, 1)\n",
        "    axes[0, 0].set_xticks(range(env.num_gpus))\n",
        "\n",
        "    # 2. Queue plot (top middle)\n",
        "    test_env = AdapterEnv(env.arrivals_by_time, num_adapters=env.num_adapters, num_gpus=env.num_gpus)\n",
        "    test_state, _ = test_env.reset()\n",
        "    tracked_queue = [len(test_env.queue)]\n",
        "    tracked_domains = []  # Track which adapter is needed\n",
        "\n",
        "    # Replay to track queue and domain\n",
        "    for i, action in enumerate(actions_history):\n",
        "        # Record domain before step\n",
        "        if test_env.queue:\n",
        "            tracked_domains.append(test_env.queue[0]['adapter_id'])\n",
        "        else:\n",
        "            tracked_domains.append(-1)  # No task\n",
        "\n",
        "        test_state, reward, done, _, _ = test_env.step(action)\n",
        "        tracked_queue.append(len(test_env.queue))\n",
        "\n",
        "    # Add final domain state\n",
        "    if test_env.queue:\n",
        "        tracked_domains.append(test_env.queue[0]['adapter_id'])\n",
        "    else:\n",
        "        tracked_domains.append(-1)\n",
        "\n",
        "    queue_line, = axes[0, 1].plot([], [], 'r-', linewidth=2, marker='o', markersize=4)\n",
        "    axes[0, 1].set_title('Queue Length Over Time')\n",
        "    axes[0, 1].set_xlabel('Step')\n",
        "    axes[0, 1].set_ylabel('Queue Length')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    max_queue = max(tracked_queue) if tracked_queue else 20\n",
        "    axes[0, 1].set_ylim(0, max_queue * 1.1 if max_queue > 0 else 20)\n",
        "\n",
        "    # 3. REWARD PLOT (top right) - NEW\n",
        "    reward_line, = axes[0, 2].plot([], [], 'g-', linewidth=2, marker='s', markersize=4)\n",
        "    axes[0, 2].set_title('Cumulative Reward')\n",
        "    axes[0, 2].set_xlabel('Step')\n",
        "    axes[0, 2].set_ylabel('Total Reward')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    min_reward = min(cumulative_rewards)\n",
        "    max_reward = max(cumulative_rewards)\n",
        "    reward_range = max_reward - min_reward\n",
        "    axes[0, 2].set_ylim(min_reward - 0.1*reward_range, max_reward + 0.1*reward_range)\n",
        "\n",
        "    # 4. Action distribution (bottom left)\n",
        "    action_types = ['Load', 'Evict', 'Reuse', 'Send']\n",
        "    action_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    action_counts = [0, 0, 0, 0]\n",
        "    action_bars = axes[1, 0].bar(action_types, action_counts, color=action_colors)\n",
        "    axes[1, 0].set_title('Action Types Taken')\n",
        "    axes[1, 0].set_ylabel('Count')\n",
        "    axes[1, 0].set_ylim(0, max(5, len(actions_history)//4))\n",
        "\n",
        "    # 5. DOMAIN PLOT (bottom middle) - NEW\n",
        "    # Show which adapter domain is being requested over time\n",
        "    domain_colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
        "    domain_line, = axes[1, 1].plot([], [], 'o-', linewidth=2, markersize=6,\n",
        "                                   color='purple', alpha=0.7)\n",
        "    axes[1, 1].set_title('Requested Adapter Domain')\n",
        "    axes[1, 1].set_xlabel('Step')\n",
        "    axes[1, 1].set_ylabel('Adapter ID')\n",
        "    axes[1, 1].set_ylim(-1.5, env.num_adapters - 0.5)\n",
        "    axes[1, 1].set_yticks(range(-1, env.num_adapters))\n",
        "    axes[1, 1].set_yticklabels(['None'] + [f'A{i}' for i in range(env.num_adapters)])\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Text info (bottom right)\n",
        "    info_text = axes[1, 2].text(0.05, 0.95, '', fontsize=10,\n",
        "                                transform=axes[1, 2].transAxes,\n",
        "                                verticalalignment='top',\n",
        "                                fontfamily='monospace',\n",
        "                                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9))\n",
        "    axes[1, 2].axis('off')\n",
        "    axes[1, 2].set_title('Step Details')\n",
        "\n",
        "    fig.suptitle('Adapter Management Agent - Episode Simulation',\n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "    def update(frame):\n",
        "        if frame >= len(states_history):\n",
        "            return\n",
        "\n",
        "        state = states_history[frame]\n",
        "        loaded_adapters = np.array(state[0]).reshape(env.num_gpus, env.num_adapters)\n",
        "        free_vram = np.array(state[1])\n",
        "\n",
        "        # 1. Update VRAM bars\n",
        "        for i, bar in enumerate(vram_bars):\n",
        "            if i < len(free_vram):\n",
        "                bar.set_height(free_vram[i])\n",
        "\n",
        "        # 2. Update queue plot\n",
        "        if frame < len(tracked_queue):\n",
        "            queue_data = tracked_queue[:frame+1]\n",
        "            queue_line.set_data(range(len(queue_data)), queue_data)\n",
        "            axes[0, 1].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "        # 3. Update REWARD plot - NEW\n",
        "        if frame < len(cumulative_rewards):\n",
        "            reward_data = cumulative_rewards[:frame+1]\n",
        "            reward_line.set_data(range(len(reward_data)), reward_data)\n",
        "            axes[0, 2].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "        # 4. Update action counts\n",
        "        if frame > 0 and frame-1 < len(actions_history):\n",
        "            action = actions_history[frame-1]\n",
        "            action_type = action // (env.num_adapters * env.num_gpus)\n",
        "            if action_type < len(action_counts):\n",
        "                action_counts[action_type] += 1\n",
        "                for i, bar in enumerate(action_bars):\n",
        "                    bar.set_height(action_counts[i])\n",
        "                max_count = max(action_counts) if action_counts else 1\n",
        "                axes[1, 0].set_ylim(0, max_count * 1.2)\n",
        "\n",
        "        # 5. Update DOMAIN plot - NEW\n",
        "        if frame < len(tracked_domains):\n",
        "            domain_data = tracked_domains[:frame+1]\n",
        "            # Use different colors for different domains\n",
        "            colors = [domain_colors[d % len(domain_colors)] if d >= 0 else 'gray'\n",
        "                     for d in domain_data]\n",
        "            domain_line.set_data(range(len(domain_data)), domain_data)\n",
        "            # Update scatter colors\n",
        "            axes[1, 1].clear()\n",
        "            axes[1, 1].set_title('Requested Adapter Domain')\n",
        "            axes[1, 1].set_xlabel('Step')\n",
        "            axes[1, 1].set_ylabel('Adapter ID')\n",
        "            axes[1, 1].set_ylim(-1.5, env.num_adapters - 0.5)\n",
        "            axes[1, 1].set_yticks(range(-1, env.num_adapters))\n",
        "            axes[1, 1].set_yticklabels(['None'] + [f'A{i}' for i in range(env.num_adapters)])\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "            axes[1, 1].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "            # Plot with colors\n",
        "            for i, (x, y) in enumerate(zip(range(len(domain_data)), domain_data)):\n",
        "                color = domain_colors[y % len(domain_colors)] if y >= 0 else 'gray'\n",
        "                axes[1, 1].plot(x, y, 'o', color=color, markersize=8, alpha=0.7)\n",
        "                if i > 0:\n",
        "                    prev_y = domain_data[i-1]\n",
        "                    axes[1, 1].plot([i-1, i], [prev_y, y], 'k-', alpha=0.3, linewidth=1)\n",
        "\n",
        "        # 6. Update info text\n",
        "        info_str = f\"Step: {frame}\\n\"\n",
        "        info_str += f\"Free VRAM: {[f'{v:.2f}' for v in free_vram]}\\n\"\n",
        "        if frame < len(tracked_queue):\n",
        "            info_str += f\"Queue: {tracked_queue[frame]}\\n\"\n",
        "        if frame < len(tracked_domains) and tracked_domains[frame] >= 0:\n",
        "            info_str += f\"Need: Adapter {tracked_domains[frame]}\\n\"\n",
        "\n",
        "        if frame > 0 and frame-1 < len(actions_history):\n",
        "            action = actions_history[frame-1]\n",
        "            action_type = action // (env.num_adapters * env.num_gpus)\n",
        "            rem = action % (env.num_adapters * env.num_gpus)\n",
        "            adapter_id = rem // env.num_gpus\n",
        "            gpu_id = rem % env.num_gpus\n",
        "\n",
        "            action_names = ['Load', 'Evict', 'Reuse', 'Send']\n",
        "            info_str += f\"\\nLast: {action_names[action_type]} A{adapter_id}â†’G{gpu_id}\\n\"\n",
        "            info_str += f\"Reward: {rewards_history[frame-1]:+.2f}\\n\"\n",
        "            info_str += f\"Total: {cumulative_rewards[frame]:.2f}\"\n",
        "\n",
        "        info_text.set_text(info_str)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        return queue_line, reward_line, domain_line, *vram_bars, *action_bars, info_text\n",
        "\n",
        "    anim = FuncAnimation(fig, update, frames=len(states_history),\n",
        "                        interval=600, blit=False, repeat=False)\n",
        "\n",
        "    return HTML(anim.to_jshtml())\n",
        "print(\"\\n  Generating animation...\")\n",
        "animation = simple_animate_episode(Q_table, env, max_steps=100)\n",
        "display(animation)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xjnEAUHDmx4I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#heuristics results:\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def simple_animate_episode_heuristic(env, max_steps=30):\n",
        "    \"\"\"Create animation for heuristic (greedy) policy results\"\"\"\n",
        "\n",
        "    # Run episode with heuristic and collect data\n",
        "    state, _ = env.reset()\n",
        "    states_history = [state]\n",
        "    actions_history = []\n",
        "    rewards_history = []\n",
        "    cumulative_rewards = [0]\n",
        "    action_names_history = []  # Store action names for display\n",
        "\n",
        "    A = env.num_adapters * env.num_gpus\n",
        "    G = env.num_gpus\n",
        "\n",
        "    done = False\n",
        "    step = 0\n",
        "\n",
        "    while not done and step < max_steps:\n",
        "        # Heuristic logic (same as test_env_heuristic)\n",
        "        action = 0\n",
        "        action_type_name = \"WAIT\"\n",
        "        adapter_id = 0\n",
        "        gpu_id = 0\n",
        "\n",
        "        if env.queue:\n",
        "            next_adapter = env.queue[0]['adapter_id']\n",
        "            loaded = np.array(state[0]).reshape(env.num_gpus, env.num_adapters)\n",
        "\n",
        "            # 1. SEND if possible\n",
        "            sent = False\n",
        "            for g in range(env.num_gpus):\n",
        "                if loaded[g, next_adapter] == 1:\n",
        "                    action = 3 * A + next_adapter * G + g\n",
        "                    action_type_name = \"SEND\"\n",
        "                    adapter_id = next_adapter\n",
        "                    gpu_id = g\n",
        "                    sent = True\n",
        "                    break\n",
        "\n",
        "            # 2. LOAD if needed\n",
        "            if not sent:\n",
        "                cost = env.adapter_vram[next_adapter]\n",
        "                for g in range(env.num_gpus):\n",
        "                    if env.free_vram[g] >= cost:\n",
        "                        action = 0 * A + next_adapter * G + g\n",
        "                        action_type_name = \"LOAD\"\n",
        "                        adapter_id = next_adapter\n",
        "                        gpu_id = g\n",
        "                        sent = True\n",
        "                        break\n",
        "\n",
        "            # 3. EVICT fallback\n",
        "            if not sent:\n",
        "                for g in range(env.num_gpus):\n",
        "                    for a in range(env.num_adapters):\n",
        "                        if loaded[g, a] == 1:\n",
        "                            action = 1 * A + a * G + g\n",
        "                            action_type_name = \"EVICT\"\n",
        "                            adapter_id = a\n",
        "                            gpu_id = g\n",
        "                            sent = True\n",
        "                            break\n",
        "                    if sent:\n",
        "                        break\n",
        "\n",
        "        # Record action info\n",
        "        actions_history.append(action)\n",
        "        action_names_history.append((action_type_name, adapter_id, gpu_id))\n",
        "\n",
        "        # Step environment\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        states_history.append(state)\n",
        "        rewards_history.append(reward)\n",
        "        cumulative_rewards.append(cumulative_rewards[-1] + reward)\n",
        "        step += 1\n",
        "\n",
        "    # Track queue and domain history (same as Q-learning version)\n",
        "    test_env = AdapterEnv(env.arrivals_by_time, num_adapters=env.num_adapters, num_gpus=env.num_gpus)\n",
        "    test_state, _ = test_env.reset()\n",
        "    tracked_queue = [len(test_env.queue)]\n",
        "    tracked_domains = []\n",
        "\n",
        "    for i, action in enumerate(actions_history):\n",
        "        if test_env.queue:\n",
        "            tracked_domains.append(test_env.queue[0]['adapter_id'])\n",
        "        else:\n",
        "            tracked_domains.append(-1)\n",
        "        test_state, _, done, _, _ = test_env.step(action)\n",
        "        tracked_queue.append(len(test_env.queue))\n",
        "\n",
        "    if test_env.queue:\n",
        "        tracked_domains.append(test_env.queue[0]['adapter_id'])\n",
        "    else:\n",
        "        tracked_domains.append(-1)\n",
        "\n",
        "    # Create figure with 2x3 grid\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "    # 1. VRAM bar chart (top left)\n",
        "    vram_bars = axes[0, 0].bar(range(env.num_gpus), [0] * env.num_gpus,\n",
        "                               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:env.num_gpus])\n",
        "    axes[0, 0].set_title('GPU VRAM Usage (Heuristic)')\n",
        "    axes[0, 0].set_xlabel('GPU ID')\n",
        "    axes[0, 0].set_ylabel('Free VRAM')\n",
        "    axes[0, 0].set_ylim(0, 1)\n",
        "    axes[0, 0].set_xticks(range(env.num_gpus))\n",
        "\n",
        "    # 2. Queue plot (top middle)\n",
        "    queue_line, = axes[0, 1].plot([], [], 'r-', linewidth=2, marker='o', markersize=4)\n",
        "    axes[0, 1].set_title('Queue Length Over Time')\n",
        "    axes[0, 1].set_xlabel('Step')\n",
        "    axes[0, 1].set_ylabel('Queue Length')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    max_queue = max(tracked_queue) if tracked_queue else 20\n",
        "    axes[0, 1].set_ylim(0, max_queue * 1.1 if max_queue > 0 else 20)\n",
        "\n",
        "    # 3. REWARD PLOT (top right)\n",
        "    reward_line, = axes[0, 2].plot([], [], 'g-', linewidth=2, marker='s', markersize=4)\n",
        "    axes[0, 2].set_title('Cumulative Reward (Heuristic)')\n",
        "    axes[0, 2].set_xlabel('Step')\n",
        "    axes[0, 2].set_ylabel('Total Reward')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    min_reward = min(cumulative_rewards)\n",
        "    max_reward = max(cumulative_rewards)\n",
        "    reward_range = max_reward - min_reward\n",
        "    axes[0, 2].set_ylim(min_reward - 0.1*reward_range, max_reward + 0.1*reward_range)\n",
        "\n",
        "    # 4. Action distribution (bottom left)\n",
        "    action_types = ['Load', 'Evict', 'Reuse', 'Send']\n",
        "    action_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    action_counts = [0, 0, 0, 0]\n",
        "    action_bars = axes[1, 0].bar(action_types, action_counts, color=action_colors)\n",
        "    axes[1, 0].set_title('Heuristic Action Types')\n",
        "    axes[1, 0].set_ylabel('Count')\n",
        "    axes[1, 0].set_ylim(0, max(5, len(actions_history)//4))\n",
        "\n",
        "    # 5. DOMAIN PLOT (bottom middle)\n",
        "    domain_colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
        "    domain_line, = axes[1, 1].plot([], [], 'o-', linewidth=2, markersize=6,\n",
        "                                   color='purple', alpha=0.7)\n",
        "    axes[1, 1].set_title('Requested Adapter Domain')\n",
        "    axes[1, 1].set_xlabel('Step')\n",
        "    axes[1, 1].set_ylabel('Adapter ID')\n",
        "    axes[1, 1].set_ylim(-1.5, env.num_adapters - 0.5)\n",
        "    axes[1, 1].set_yticks(range(-1, env.num_adapters))\n",
        "    axes[1, 1].set_yticklabels(['None'] + [f'A{i}' for i in range(env.num_adapters)])\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Text info (bottom right)\n",
        "    info_text = axes[1, 2].text(0.05, 0.95, '', fontsize=10,\n",
        "                                transform=axes[1, 2].transAxes,\n",
        "                                verticalalignment='top',\n",
        "                                fontfamily='monospace',\n",
        "                                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.9))\n",
        "    axes[1, 2].axis('off')\n",
        "    axes[1, 2].set_title('Heuristic Decision Log')\n",
        "\n",
        "    fig.suptitle('Adapter Management - Heuristic (Greedy) Policy',\n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "    def update(frame):\n",
        "        if frame >= len(states_history):\n",
        "            return\n",
        "\n",
        "        state = states_history[frame]\n",
        "        loaded_adapters = np.array(state[0]).reshape(env.num_gpus, env.num_adapters)\n",
        "        free_vram = np.array(state[1])\n",
        "\n",
        "        # 1. Update VRAM bars\n",
        "        for i, bar in enumerate(vram_bars):\n",
        "            if i < len(free_vram):\n",
        "                bar.set_height(free_vram[i])\n",
        "\n",
        "        # 2. Update queue plot\n",
        "        if frame < len(tracked_queue):\n",
        "            queue_data = tracked_queue[:frame+1]\n",
        "            queue_line.set_data(range(len(queue_data)), queue_data)\n",
        "            axes[0, 1].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "        # 3. Update REWARD plot\n",
        "        if frame < len(cumulative_rewards):\n",
        "            reward_data = cumulative_rewards[:frame+1]\n",
        "            reward_line.set_data(range(len(reward_data)), reward_data)\n",
        "            axes[0, 2].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "        # 4. Update action counts\n",
        "        if frame > 0 and frame-1 < len(actions_history):\n",
        "            action = actions_history[frame-1]\n",
        "            action_type = action // (env.num_adapters * env.num_gpus)\n",
        "            if action_type < len(action_counts):\n",
        "                action_counts[action_type] += 1\n",
        "                for i, bar in enumerate(action_bars):\n",
        "                    bar.set_height(action_counts[i])\n",
        "                max_count = max(action_counts) if action_counts else 1\n",
        "                axes[1, 0].set_ylim(0, max_count * 1.2)\n",
        "\n",
        "        # 5. Update DOMAIN plot\n",
        "        if frame < len(tracked_domains):\n",
        "            domain_data = tracked_domains[:frame+1]\n",
        "            axes[1, 1].clear()\n",
        "            axes[1, 1].set_title('Requested Adapter Domain')\n",
        "            axes[1, 1].set_xlabel('Step')\n",
        "            axes[1, 1].set_ylabel('Adapter ID')\n",
        "            axes[1, 1].set_ylim(-1.5, env.num_adapters - 0.5)\n",
        "            axes[1, 1].set_yticks(range(-1, env.num_adapters))\n",
        "            axes[1, 1].set_yticklabels(['None'] + [f'A{i}' for i in range(env.num_adapters)])\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "            axes[1, 1].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "            for i, (x, y) in enumerate(zip(range(len(domain_data)), domain_data)):\n",
        "                color = domain_colors[y % len(domain_colors)] if y >= 0 else 'gray'\n",
        "                axes[1, 1].plot(x, y, 'o', color=color, markersize=8, alpha=0.7)\n",
        "                if i > 0:\n",
        "                    prev_y = domain_data[i-1]\n",
        "                    axes[1, 1].plot([i-1, i], [prev_y, y], 'k-', alpha=0.3, linewidth=1)\n",
        "\n",
        "        # 6. Update info text with heuristic decision\n",
        "        info_str = f\"Step: {frame}\\n\"\n",
        "        info_str += f\"Free VRAM: {[f'{v:.2f}' for v in free_vram]}\\n\"\n",
        "        if frame < len(tracked_queue):\n",
        "            info_str += f\"Queue: {tracked_queue[frame]}\\n\"\n",
        "        if frame < len(tracked_domains) and tracked_domains[frame] >= 0:\n",
        "            info_str += f\"Need: Adapter {tracked_domains[frame]}\\n\"\n",
        "\n",
        "        if frame > 0 and frame-1 < len(action_names_history):\n",
        "            action_name, adapter_id, gpu_id = action_names_history[frame-1]\n",
        "            info_str += f\"\\nHeuristic: {action_name}\\n\"\n",
        "            info_str += f\"  A{adapter_id} â†’ GPU{gpu_id}\\n\"\n",
        "            info_str += f\"Reward: {rewards_history[frame-1]:+.2f}\\n\"\n",
        "            info_str += f\"Total: {cumulative_rewards[frame]:.2f}\"\n",
        "\n",
        "        info_text.set_text(info_str)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        return queue_line, reward_line, domain_line, *vram_bars, *action_bars, info_text\n",
        "\n",
        "    anim = FuncAnimation(fig, update, frames=len(states_history),\n",
        "                        interval=600, blit=False, repeat=False)\n",
        "\n",
        "    return HTML(anim.to_jshtml())\n",
        "\n",
        "# === USAGE ===\n",
        "# After running test_env_heuristic or with your env:\n",
        "print(\"\\n Generating heuristic animation...\")\n",
        "animation_heuristic = simple_animate_episode_heuristic(env, max_steps=50)\n",
        "display(animation_heuristic)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RQj5xIbRmx4L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "from collections import deque\n",
        "\n",
        "def simple_animate_episode(Q, env, max_steps=100):\n",
        "    \"\"\"Create a simple animated visualization of a single episode with reward and domain tracking\"\"\"\n",
        "\n",
        "    # Run episode and collect data\n",
        "    state, _ = env.reset()\n",
        "    states_history = [state]\n",
        "    actions_history = []\n",
        "    rewards_history = []\n",
        "    cumulative_rewards = [0]  # Start with 0\n",
        "\n",
        "    done = False\n",
        "    step = 0\n",
        "\n",
        "    while not done and step < max_steps:\n",
        "        if state in Q:\n",
        "            action = np.argmax(Q[state])\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        actions_history.append(action)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        states_history.append(state)\n",
        "        rewards_history.append(reward)\n",
        "        cumulative_rewards.append(cumulative_rewards[-1] + reward)  # Track cumulative\n",
        "        step += 1\n",
        "\n",
        "    # Create figure with 3x2 grid (6 plots) - Removed empty diagrams\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(15, 14))  # Increased height\n",
        "\n",
        "    # 1. VRAM bar chart (row 1, col 1)\n",
        "    vram_bars = axes[0, 0].bar(range(env.num_gpus), [0] * env.num_gpus,\n",
        "                               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:env.num_gpus])\n",
        "    axes[0, 0].set_title('GPU VRAM Usage', fontsize=12, pad=10)\n",
        "    axes[0, 0].set_xlabel('GPU ID', fontsize=10)\n",
        "    axes[0, 0].set_ylabel('Free VRAM', fontsize=10)\n",
        "    axes[0, 0].set_ylim(0, 1)\n",
        "    axes[0, 0].set_xticks(range(env.num_gpus))\n",
        "\n",
        "    # 2. Queue plot (row 1, col 2)\n",
        "    test_env = AdapterEnv(env.arrivals_by_time, num_adapters=env.num_adapters, num_gpus=env.num_gpus)\n",
        "    test_state, _ = test_env.reset()\n",
        "    tracked_queue = [len(test_env.queue)]\n",
        "    tracked_domains = []  # Track which adapter is needed\n",
        "\n",
        "    # Replay to track queue and domain\n",
        "    for i, action in enumerate(actions_history):\n",
        "        # Record domain before step\n",
        "        if test_env.queue:\n",
        "            tracked_domains.append(test_env.queue[0]['adapter_id'])\n",
        "        else:\n",
        "            tracked_domains.append(-1)  # No task\n",
        "\n",
        "        test_state, reward, done, _, _ = test_env.step(action)\n",
        "        tracked_queue.append(len(test_env.queue))\n",
        "\n",
        "    # Add final domain state\n",
        "    if test_env.queue:\n",
        "        tracked_domains.append(test_env.queue[0]['adapter_id'])\n",
        "    else:\n",
        "        tracked_domains.append(-1)\n",
        "\n",
        "    queue_line, = axes[0, 1].plot([], [], 'r-', linewidth=2, marker='o', markersize=4)\n",
        "    axes[0, 1].set_title('Queue Length Over Time', fontsize=12, pad=10)\n",
        "    axes[0, 1].set_xlabel('Step', fontsize=10)\n",
        "    axes[0, 1].set_ylabel('Queue Length', fontsize=10)\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    max_queue = max(tracked_queue) if tracked_queue else 20\n",
        "    axes[0, 1].set_ylim(0, max_queue * 1.1 if max_queue > 0 else 20)\n",
        "\n",
        "    # 3. Cumulative Reward plot (row 2, col 1)\n",
        "    cumulative_line, = axes[1, 0].plot([], [], 'g-', linewidth=2, marker='s', markersize=4)\n",
        "    axes[1, 0].set_title('Cumulative Reward', fontsize=12, pad=10)\n",
        "    axes[1, 0].set_xlabel('Step', fontsize=10)\n",
        "    axes[1, 0].set_ylabel('Total Reward', fontsize=10)\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. REWARD PER STEP plot (row 2, col 2)\n",
        "    step_reward_line, = axes[1, 1].plot([], [], 'b-', linewidth=2, marker='D', markersize=4)\n",
        "    axes[1, 1].set_title('Reward Per Step', fontsize=12, pad=10)\n",
        "    axes[1, 1].set_xlabel('Step', fontsize=10)\n",
        "    axes[1, 1].set_ylabel('Step Reward', fontsize=10)\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    axes[1, 1].axhline(y=0, color='gray', linestyle='-', alpha=0.3, linewidth=1)\n",
        "\n",
        "    # Color code for positive/negative rewards\n",
        "    step_reward_pos, = axes[1, 1].plot([], [], 'go', markersize=6, alpha=0.7, label='Positive')\n",
        "    step_reward_neg, = axes[1, 1].plot([], [], 'ro', markersize=6, alpha=0.7, label='Negative')\n",
        "    axes[1, 1].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "    # 5. Action distribution (row 3, col 1)\n",
        "    action_types = ['Load', 'Evict', 'Reuse', 'Send']\n",
        "    action_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    action_counts = [0, 0, 0, 0]\n",
        "    action_bars = axes[2, 0].bar(action_types, action_counts, color=action_colors)\n",
        "    axes[2, 0].set_title('Action Types Taken', fontsize=12, pad=10)\n",
        "    axes[2, 0].set_xlabel('Action Type', fontsize=10)\n",
        "    axes[2, 0].set_ylabel('Count', fontsize=10)\n",
        "    axes[2, 0].set_ylim(0, max(5, len(actions_history)//4))\n",
        "\n",
        "    # 6. DOMAIN PLOT - Requested adapters (row 3, col 2)\n",
        "    domain_colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
        "    requested_line, = axes[2, 1].plot([], [], 'o-', linewidth=2, markersize=6,\n",
        "                                     color='purple', alpha=0.7, label='Requested')\n",
        "    axes[2, 1].set_title('Requested Adapter', fontsize=12, pad=10)\n",
        "    axes[2, 1].set_xlabel('Step', fontsize=10)\n",
        "    axes[2, 1].set_ylabel('Adapter ID', fontsize=10)\n",
        "    axes[2, 1].set_ylim(-1.5, env.num_adapters - 0.5)\n",
        "    axes[2, 1].set_yticks(range(-1, env.num_adapters))\n",
        "    axes[2, 1].set_yticklabels(['None'] + [f'A{i}' for i in range(env.num_adapters)])\n",
        "    axes[2, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 7. Text info - Create a separate axis on the right side\n",
        "    fig.subplots_adjust(right=0.85)  # Make room for text on the right\n",
        "    info_ax = fig.add_axes([0.87, 0.1, 0.12, 0.8])  # [left, bottom, width, height]\n",
        "    info_ax.axis('off')\n",
        "    info_text = info_ax.text(0.02, 0.98, '', fontsize=9,\n",
        "                             verticalalignment='top',\n",
        "                             fontfamily='monospace',\n",
        "                             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9))\n",
        "\n",
        "    fig.suptitle('Adapter Management Agent - Episode Simulation',\n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "    def update(frame):\n",
        "        if frame >= len(states_history):\n",
        "            return\n",
        "\n",
        "        state = states_history[frame]\n",
        "        loaded_adapters = np.array(state[0]).reshape(env.num_gpus, env.num_adapters)\n",
        "        free_vram = np.array(state[1])\n",
        "\n",
        "        # 1. Update VRAM bars\n",
        "        for i, bar in enumerate(vram_bars):\n",
        "            if i < len(free_vram):\n",
        "                bar.set_height(free_vram[i])\n",
        "\n",
        "        # 2. Update queue plot\n",
        "        if frame < len(tracked_queue):\n",
        "            queue_data = tracked_queue[:frame+1]\n",
        "            queue_line.set_data(range(len(queue_data)), queue_data)\n",
        "            axes[0, 1].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "        # 3. Update Cumulative Reward plot\n",
        "        if frame < len(cumulative_rewards):\n",
        "            cumulative_data = cumulative_rewards[:frame+1]\n",
        "            cumulative_line.set_data(range(len(cumulative_data)), cumulative_data)\n",
        "            axes[1, 0].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "            # Auto-scale y-axis for cumulative reward\n",
        "            if len(cumulative_data) > 0:\n",
        "                min_val = min(cumulative_data)\n",
        "                max_val = max(cumulative_data)\n",
        "                if min_val != max_val:\n",
        "                    axes[1, 0].set_ylim(min_val - 0.1*(max_val-min_val),\n",
        "                                       max_val + 0.1*(max_val-min_val))\n",
        "\n",
        "        # 4. Update REWARD PER STEP plot\n",
        "        if frame > 0:\n",
        "            # Plot all step rewards\n",
        "            step_data = rewards_history[:frame]\n",
        "            x_data = range(1, frame+1)\n",
        "\n",
        "            # Clear and redraw\n",
        "            axes[1, 1].clear()\n",
        "            axes[1, 1].set_title('Reward Per Step', fontsize=12, pad=10)\n",
        "            axes[1, 1].set_xlabel('Step', fontsize=10)\n",
        "            axes[1, 1].set_ylabel('Step Reward', fontsize=10)\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "            axes[1, 1].axhline(y=0, color='gray', linestyle='-', alpha=0.3, linewidth=1)\n",
        "\n",
        "            # Separate positive and negative rewards\n",
        "            pos_x, pos_y = [], []\n",
        "            neg_x, neg_y = [], []\n",
        "\n",
        "            for i, reward in enumerate(step_data):\n",
        "                if reward >= 0:\n",
        "                    pos_x.append(i+1)\n",
        "                    pos_y.append(reward)\n",
        "                else:\n",
        "                    neg_x.append(i+1)\n",
        "                    neg_y.append(reward)\n",
        "\n",
        "            # Plot with different colors\n",
        "            if pos_x:\n",
        "                axes[1, 1].plot(pos_x, pos_y, 'go', markersize=6, alpha=0.7, label='Positive')\n",
        "            if neg_x:\n",
        "                axes[1, 1].plot(neg_x, neg_y, 'ro', markersize=6, alpha=0.7, label='Negative')\n",
        "\n",
        "            # Add connecting lines\n",
        "            if len(step_data) > 1:\n",
        "                axes[1, 1].plot(x_data, step_data, 'b-', alpha=0.3, linewidth=1)\n",
        "\n",
        "            axes[1, 1].set_xlim(0.5, max(1.5, frame+0.5))\n",
        "            if step_data:\n",
        "                min_reward = min(step_data)\n",
        "                max_reward = max(step_data)\n",
        "                reward_range = max_reward - min_reward\n",
        "                if reward_range > 0:\n",
        "                    axes[1, 1].set_ylim(min_reward - 0.1*reward_range, max_reward + 0.1*reward_range)\n",
        "\n",
        "            axes[1, 1].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "        # 5. Update action counts\n",
        "        if frame > 0 and frame-1 < len(actions_history):\n",
        "            action = actions_history[frame-1]\n",
        "            action_type = action // (env.num_adapters * env.num_gpus)\n",
        "            if action_type < len(action_counts):\n",
        "                action_counts[action_type] += 1\n",
        "                for i, bar in enumerate(action_bars):\n",
        "                    bar.set_height(action_counts[i])\n",
        "                max_count = max(action_counts) if action_counts else 1\n",
        "                axes[2, 0].set_ylim(0, max_count * 1.2)\n",
        "\n",
        "        # 6. Update DOMAIN plot (Requested adapters)\n",
        "        if frame < len(tracked_domains):\n",
        "            domain_data = tracked_domains[:frame+1]\n",
        "            requested_line.set_data(range(len(domain_data)), domain_data)\n",
        "            axes[2, 1].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "            # Color code points\n",
        "            axes[2, 1].clear()\n",
        "            axes[2, 1].set_title('Requested Adapter', fontsize=12, pad=10)\n",
        "            axes[2, 1].set_xlabel('Step', fontsize=10)\n",
        "            axes[2, 1].set_ylabel('Adapter ID', fontsize=10)\n",
        "            axes[2, 1].set_ylim(-1.5, env.num_adapters - 0.5)\n",
        "            axes[2, 1].set_yticks(range(-1, env.num_adapters))\n",
        "            axes[2, 1].set_yticklabels(['None'] + [f'A{i}' for i in range(env.num_adapters)])\n",
        "            axes[2, 1].grid(True, alpha=0.3)\n",
        "            axes[2, 1].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "            for i, adapter in enumerate(domain_data):\n",
        "                color = domain_colors[adapter % len(domain_colors)] if adapter >= 0 else 'gray'\n",
        "                axes[2, 1].plot(i, adapter, 'o', color=color, markersize=8, alpha=0.7)\n",
        "                if i > 0:\n",
        "                    prev_adapter = domain_data[i-1]\n",
        "                    axes[2, 1].plot([i-1, i], [prev_adapter, adapter], 'k-', alpha=0.3, linewidth=1)\n",
        "\n",
        "        # 7. Update info text\n",
        "        info_str = f\"Step: {frame}\\n\"\n",
        "        info_str += f\"Free VRAM: {[f'{v:.2f}' for v in free_vram]}\\n\"\n",
        "        if frame < len(tracked_queue):\n",
        "            info_str += f\"Queue: {tracked_queue[frame]}\\n\"\n",
        "        if frame < len(tracked_domains) and tracked_domains[frame] >= 0:\n",
        "            info_str += f\"Need: Adapter {tracked_domains[frame]}\\n\"\n",
        "\n",
        "        if frame > 0 and frame-1 < len(actions_history):\n",
        "            action = actions_history[frame-1]\n",
        "            action_type = action // (env.num_adapters * env.num_gpus)\n",
        "            rem = action % (env.num_adapters * env.num_gpus)\n",
        "            adapter_id = rem // env.num_gpus\n",
        "            gpu_id = rem % env.num_gpus\n",
        "\n",
        "            action_names = ['Load', 'Evict', 'Reuse', 'Send']\n",
        "            info_str += f\"\\nLast Action:\\n\"\n",
        "            info_str += f\"  {action_names[action_type]} A{adapter_id}â†’G{gpu_id}\\n\"\n",
        "            info_str += f\"  Step Reward: {rewards_history[frame-1]:+.2f}\\n\"\n",
        "            info_str += f\"  Total: {cumulative_rewards[frame]:.2f}\"\n",
        "\n",
        "            # Add reward statistics\n",
        "            if frame > 1:\n",
        "                avg_reward = np.mean(rewards_history[:frame])\n",
        "                pos_rewards = sum(1 for r in rewards_history[:frame] if r > 0)\n",
        "                neg_rewards = sum(1 for r in rewards_history[:frame] if r < 0)\n",
        "                info_str += f\"\\n\\nReward Stats:\\n\"\n",
        "                info_str += f\"  Avg/step: {avg_reward:.2f}\\n\"\n",
        "                info_str += f\"  Positive: {pos_rewards}\\n\"\n",
        "                info_str += f\"  Negative: {neg_rewards}\"\n",
        "\n",
        "        info_text.set_text(info_str)\n",
        "\n",
        "        # Adjust layout to prevent overlap\n",
        "        plt.tight_layout(rect=[0, 0, 0.85, 0.96])  # Leave right 15% for text\n",
        "\n",
        "        return (queue_line, cumulative_line, step_reward_line,\n",
        "                requested_line, *vram_bars, info_text)\n",
        "\n",
        "    anim = FuncAnimation(fig, update, frames=len(states_history),\n",
        "                        interval=600, blit=False, repeat=False)\n",
        "\n",
        "    return HTML(anim.to_jshtml())\n",
        "\n",
        "print(\"\\nGenerating animation...\")\n",
        "animation = simple_animate_episode(Q_table, env, max_steps=100)\n",
        "display(animation)"
      ],
      "metadata": {
        "trusted": true,
        "id": "3szNl5M0mx4M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#heuristics results:\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def simple_animate_episode_heuristic(env, max_steps=100):\n",
        "    \"\"\"Create animation for heuristic (greedy) policy results\"\"\"\n",
        "\n",
        "    # Run episode with heuristic and collect data\n",
        "    state, _ = env.reset()\n",
        "    states_history = [state]\n",
        "    actions_history = []\n",
        "    rewards_history = []\n",
        "    cumulative_rewards = [0]\n",
        "    action_names_history = []  # Store action names for display\n",
        "\n",
        "    A = env.num_adapters * env.num_gpus\n",
        "    G = env.num_gpus\n",
        "\n",
        "    done = False\n",
        "    step = 0\n",
        "\n",
        "    while not done and step < max_steps:\n",
        "        # Heuristic logic (same as test_env_heuristic)\n",
        "        action = 0\n",
        "        action_type_name = \"WAIT\"\n",
        "        adapter_id = 0\n",
        "        gpu_id = 0\n",
        "\n",
        "        if env.queue:\n",
        "            next_adapter = env.queue[0]['adapter_id']\n",
        "            loaded = np.array(state[0]).reshape(env.num_gpus, env.num_adapters)\n",
        "\n",
        "            # 1. SEND if possible\n",
        "            sent = False\n",
        "            for g in range(env.num_gpus):\n",
        "                if loaded[g, next_adapter] == 1:\n",
        "                    action = 3 * A + next_adapter * G + g\n",
        "                    action_type_name = \"SEND\"\n",
        "                    adapter_id = next_adapter\n",
        "                    gpu_id = g\n",
        "                    sent = True\n",
        "                    break\n",
        "\n",
        "            # 2. LOAD if needed\n",
        "            if not sent:\n",
        "                cost = env.adapter_vram[next_adapter]\n",
        "                for g in range(env.num_gpus):\n",
        "                    if env.free_vram[g] >= cost:\n",
        "                        action = 0 * A + next_adapter * G + g\n",
        "                        action_type_name = \"LOAD\"\n",
        "                        adapter_id = next_adapter\n",
        "                        gpu_id = g\n",
        "                        sent = True\n",
        "                        break\n",
        "\n",
        "            # 3. EVICT fallback\n",
        "            if not sent:\n",
        "                for g in range(env.num_gpus):\n",
        "                    for a in range(env.num_adapters):\n",
        "                        if loaded[g, a] == 1:\n",
        "                            action = 1 * A + a * G + g\n",
        "                            action_type_name = \"EVICT\"\n",
        "                            adapter_id = a\n",
        "                            gpu_id = g\n",
        "                            sent = True\n",
        "                            break\n",
        "                    if sent:\n",
        "                        break\n",
        "\n",
        "        # Record action info\n",
        "        actions_history.append(action)\n",
        "        action_names_history.append((action_type_name, adapter_id, gpu_id))\n",
        "\n",
        "        # Step environment\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        states_history.append(state)\n",
        "        rewards_history.append(reward)\n",
        "        cumulative_rewards.append(cumulative_rewards[-1] + reward)\n",
        "        step += 1\n",
        "\n",
        "    # Track queue and domain history (same as Q-learning version)\n",
        "    test_env = AdapterEnv(env.arrivals_by_time, num_adapters=env.num_adapters, num_gpus=env.num_gpus)\n",
        "    test_state, _ = test_env.reset()\n",
        "    tracked_queue = [len(test_env.queue)]\n",
        "    tracked_domains = []\n",
        "\n",
        "    for i, action in enumerate(actions_history):\n",
        "        if test_env.queue:\n",
        "            tracked_domains.append(test_env.queue[0]['adapter_id'])\n",
        "        else:\n",
        "            tracked_domains.append(-1)\n",
        "        test_state, _, done, _, _ = test_env.step(action)\n",
        "        tracked_queue.append(len(test_env.queue))\n",
        "\n",
        "    if test_env.queue:\n",
        "        tracked_domains.append(test_env.queue[0]['adapter_id'])\n",
        "    else:\n",
        "        tracked_domains.append(-1)\n",
        "\n",
        "    # Create figure with 3x3 grid to accommodate all plots including Reward Per Step\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "\n",
        "    # 1. VRAM bar chart (row 1, col 1)\n",
        "    vram_bars = axes[0, 0].bar(range(env.num_gpus), [0] * env.num_gpus,\n",
        "                               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:env.num_gpus])\n",
        "    axes[0, 0].set_title('GPU VRAM Usage (Heuristic)')\n",
        "    axes[0, 0].set_xlabel('GPU ID')\n",
        "    axes[0, 0].set_ylabel('Free VRAM')\n",
        "    axes[0, 0].set_ylim(0, 1)\n",
        "    axes[0, 0].set_xticks(range(env.num_gpus))\n",
        "\n",
        "    # 2. Queue plot (row 1, col 2)\n",
        "    queue_line, = axes[0, 1].plot([], [], 'r-', linewidth=2, marker='o', markersize=4)\n",
        "    axes[0, 1].set_title('Queue Length Over Time')\n",
        "    axes[0, 1].set_xlabel('Step')\n",
        "    axes[0, 1].set_ylabel('Queue Length')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    max_queue = max(tracked_queue) if tracked_queue else 20\n",
        "    axes[0, 1].set_ylim(0, max_queue * 1.1 if max_queue > 0 else 20)\n",
        "\n",
        "    # 3. REWARD PER STEP plot (row 1, col 3) - NEW!\n",
        "    step_reward_line, = axes[0, 2].plot([], [], 'b-', linewidth=2, marker='D', markersize=4)\n",
        "    axes[0, 2].set_title('Reward Per Step (Heuristic)')\n",
        "    axes[0, 2].set_xlabel('Step')\n",
        "    axes[0, 2].set_ylabel('Step Reward')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    axes[0, 2].axhline(y=0, color='gray', linestyle='-', alpha=0.3, linewidth=1)\n",
        "\n",
        "    # Color code for positive/negative rewards\n",
        "    step_reward_pos, = axes[0, 2].plot([], [], 'go', markersize=6, alpha=0.7, label='Positive')\n",
        "    step_reward_neg, = axes[0, 2].plot([], [], 'ro', markersize=6, alpha=0.7, label='Negative')\n",
        "    axes[0, 2].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "    # 4. CUMULATIVE REWARD plot (row 2, col 1)\n",
        "    cumulative_line, = axes[1, 0].plot([], [], 'g-', linewidth=2, marker='s', markersize=4)\n",
        "    axes[1, 0].set_title('Cumulative Reward (Heuristic)')\n",
        "    axes[1, 0].set_xlabel('Step')\n",
        "    axes[1, 0].set_ylabel('Total Reward')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Action distribution (row 2, col 2)\n",
        "    action_types = ['Load', 'Evict', 'Reuse', 'Send']\n",
        "    action_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    action_counts = [0, 0, 0, 0]\n",
        "    action_bars = axes[1, 1].bar(action_types, action_counts, color=action_colors)\n",
        "    axes[1, 1].set_title('Heuristic Action Types')\n",
        "    axes[1, 1].set_xlabel('Action Type')\n",
        "    axes[1, 1].set_ylabel('Count')\n",
        "    axes[1, 1].set_ylim(0, max(5, len(actions_history)//4))\n",
        "\n",
        "    # 6. DOMAIN PLOT (row 2, col 3)\n",
        "    domain_colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
        "    domain_line, = axes[1, 2].plot([], [], 'o-', linewidth=2, markersize=6,\n",
        "                                   color='purple', alpha=0.7)\n",
        "    axes[1, 2].set_title('Requested Adapter Domain')\n",
        "    axes[1, 2].set_xlabel('Step')\n",
        "    axes[1, 2].set_ylabel('Adapter ID')\n",
        "    axes[1, 2].set_ylim(-1.5, env.num_adapters - 0.5)\n",
        "    axes[1, 2].set_yticks(range(-1, env.num_adapters))\n",
        "    axes[1, 2].set_yticklabels(['None'] + [f'A{i}' for i in range(env.num_adapters)])\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # 7. Text info (row 3, col 1) - spanning 3 columns\n",
        "    info_ax = axes[2, 0]\n",
        "    info_text = info_ax.text(0.05, 0.95, '', fontsize=9,\n",
        "                             transform=info_ax.transAxes,\n",
        "                             verticalalignment='top',\n",
        "                             fontfamily='monospace',\n",
        "                             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.9))\n",
        "    info_ax.axis('off')\n",
        "    info_ax.set_title('Heuristic Decision Log')\n",
        "\n",
        "    # Hide empty subplots (row 3, col 2 and col 3)\n",
        "    axes[2, 1].axis('off')\n",
        "    axes[2, 2].axis('off')\n",
        "\n",
        "    fig.suptitle('Adapter Management - Heuristic (Greedy) Policy',\n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "    def update(frame):\n",
        "        if frame >= len(states_history):\n",
        "            return\n",
        "\n",
        "        state = states_history[frame]\n",
        "        loaded_adapters = np.array(state[0]).reshape(env.num_gpus, env.num_adapters)\n",
        "        free_vram = np.array(state[1])\n",
        "\n",
        "        # 1. Update VRAM bars\n",
        "        for i, bar in enumerate(vram_bars):\n",
        "            if i < len(free_vram):\n",
        "                bar.set_height(free_vram[i])\n",
        "\n",
        "        # 2. Update queue plot\n",
        "        if frame < len(tracked_queue):\n",
        "            queue_data = tracked_queue[:frame+1]\n",
        "            queue_line.set_data(range(len(queue_data)), queue_data)\n",
        "            axes[0, 1].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "        # 3. Update REWARD PER STEP plot\n",
        "        if frame > 0:\n",
        "            # Plot all step rewards\n",
        "            step_data = rewards_history[:frame]\n",
        "            x_data = range(1, frame+1)\n",
        "\n",
        "            # Clear and redraw\n",
        "            axes[0, 2].clear()\n",
        "            axes[0, 2].set_title('Reward Per Step (Heuristic)')\n",
        "            axes[0, 2].set_xlabel('Step')\n",
        "            axes[0, 2].set_ylabel('Step Reward')\n",
        "            axes[0, 2].grid(True, alpha=0.3)\n",
        "            axes[0, 2].axhline(y=0, color='gray', linestyle='-', alpha=0.3, linewidth=1)\n",
        "\n",
        "            # Separate positive and negative rewards\n",
        "            pos_x, pos_y = [], []\n",
        "            neg_x, neg_y = [], []\n",
        "\n",
        "            for i, reward in enumerate(step_data):\n",
        "                if reward >= 0:\n",
        "                    pos_x.append(i+1)\n",
        "                    pos_y.append(reward)\n",
        "                else:\n",
        "                    neg_x.append(i+1)\n",
        "                    neg_y.append(reward)\n",
        "\n",
        "            # Plot with different colors\n",
        "            if pos_x:\n",
        "                axes[0, 2].plot(pos_x, pos_y, 'go', markersize=6, alpha=0.7, label='Positive')\n",
        "            if neg_x:\n",
        "                axes[0, 2].plot(neg_x, neg_y, 'ro', markersize=6, alpha=0.7, label='Negative')\n",
        "\n",
        "            # Add connecting lines\n",
        "            if len(step_data) > 1:\n",
        "                axes[0, 2].plot(x_data, step_data, 'b-', alpha=0.3, linewidth=1)\n",
        "\n",
        "            axes[0, 2].set_xlim(0.5, max(1.5, frame+0.5))\n",
        "            if step_data:\n",
        "                min_reward = min(step_data)\n",
        "                max_reward = max(step_data)\n",
        "                reward_range = max_reward - min_reward\n",
        "                if reward_range > 0:\n",
        "                    axes[0, 2].set_ylim(min_reward - 0.1*reward_range, max_reward + 0.1*reward_range)\n",
        "\n",
        "            axes[0, 2].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "        # 4. Update CUMULATIVE REWARD plot\n",
        "        if frame < len(cumulative_rewards):\n",
        "            cumulative_data = cumulative_rewards[:frame+1]\n",
        "            cumulative_line.set_data(range(len(cumulative_data)), cumulative_data)\n",
        "            axes[1, 0].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "            # Auto-scale y-axis for cumulative reward\n",
        "            if len(cumulative_data) > 0:\n",
        "                min_val = min(cumulative_data)\n",
        "                max_val = max(cumulative_data)\n",
        "                if min_val != max_val:\n",
        "                    axes[1, 0].set_ylim(min_val - 0.1*(max_val-min_val),\n",
        "                                       max_val + 0.1*(max_val-min_val))\n",
        "\n",
        "        # 5. Update action counts\n",
        "        if frame > 0 and frame-1 < len(actions_history):\n",
        "            action = actions_history[frame-1]\n",
        "            action_type = action // (env.num_adapters * env.num_gpus)\n",
        "            if action_type < len(action_counts):\n",
        "                action_counts[action_type] += 1\n",
        "                for i, bar in enumerate(action_bars):\n",
        "                    bar.set_height(action_counts[i])\n",
        "                max_count = max(action_counts) if action_counts else 1\n",
        "                axes[1, 1].set_ylim(0, max_count * 1.2)\n",
        "\n",
        "        # 6. Update DOMAIN plot\n",
        "        if frame < len(tracked_domains):\n",
        "            domain_data = tracked_domains[:frame+1]\n",
        "            axes[1, 2].clear()\n",
        "            axes[1, 2].set_title('Requested Adapter Domain')\n",
        "            axes[1, 2].set_xlabel('Step')\n",
        "            axes[1, 2].set_ylabel('Adapter ID')\n",
        "            axes[1, 2].set_ylim(-1.5, env.num_adapters - 0.5)\n",
        "            axes[1, 2].set_yticks(range(-1, env.num_adapters))\n",
        "            axes[1, 2].set_yticklabels(['None'] + [f'A{i}' for i in range(env.num_adapters)])\n",
        "            axes[1, 2].grid(True, alpha=0.3)\n",
        "            axes[1, 2].set_xlim(0, max(1, len(states_history)))\n",
        "\n",
        "            for i, (x, y) in enumerate(zip(range(len(domain_data)), domain_data)):\n",
        "                color = domain_colors[y % len(domain_colors)] if y >= 0 else 'gray'\n",
        "                axes[1, 2].plot(x, y, 'o', color=color, markersize=8, alpha=0.7)\n",
        "                if i > 0:\n",
        "                    prev_y = domain_data[i-1]\n",
        "                    axes[1, 2].plot([i-1, i], [prev_y, y], 'k-', alpha=0.3, linewidth=1)\n",
        "\n",
        "        # 7. Update info text with heuristic decision\n",
        "        info_str = f\"Step: {frame}\\n\"\n",
        "        info_str += f\"Free VRAM: {[f'{v:.2f}' for v in free_vram]}\\n\"\n",
        "        if frame < len(tracked_queue):\n",
        "            info_str += f\"Queue: {tracked_queue[frame]}\\n\"\n",
        "        if frame < len(tracked_domains) and tracked_domains[frame] >= 0:\n",
        "            info_str += f\"Need: Adapter {tracked_domains[frame]}\\n\"\n",
        "\n",
        "        if frame > 0 and frame-1 < len(action_names_history):\n",
        "            action_name, adapter_id, gpu_id = action_names_history[frame-1]\n",
        "            info_str += f\"\\nHeuristic: {action_name}\\n\"\n",
        "            info_str += f\"  A{adapter_id} â†’ GPU{gpu_id}\\n\"\n",
        "            info_str += f\"Step Reward: {rewards_history[frame-1]:+.2f}\\n\"\n",
        "            info_str += f\"Total: {cumulative_rewards[frame]:.2f}\\n\"\n",
        "\n",
        "            # Add reward statistics\n",
        "            if frame > 1:\n",
        "                avg_reward = np.mean(rewards_history[:frame])\n",
        "                pos_rewards = sum(1 for r in rewards_history[:frame] if r > 0)\n",
        "                neg_rewards = sum(1 for r in rewards_history[:frame] if r < 0)\n",
        "                info_str += f\"\\nReward Stats:\\n\"\n",
        "                info_str += f\"  Avg/step: {avg_reward:.2f}\\n\"\n",
        "                info_str += f\"  Positive: {pos_rewards}\\n\"\n",
        "                info_str += f\"  Negative: {neg_rewards}\"\n",
        "\n",
        "        info_text.set_text(info_str)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        return (queue_line, step_reward_line, cumulative_line, domain_line,\n",
        "                *vram_bars, *action_bars, info_text)\n",
        "\n",
        "    anim = FuncAnimation(fig, update, frames=len(states_history),\n",
        "                        interval=600, blit=False, repeat=False)\n",
        "\n",
        "    return HTML(anim.to_jshtml())\n",
        "\n",
        "# === USAGE ===\n",
        "# After running test_env_heuristic or with your env:\n",
        "print(\"\\nGenerating heuristic animation...\")\n",
        "animation_heuristic = simple_animate_episode_heuristic(env, max_steps=100)\n",
        "display(animation_heuristic)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bvwVVMGEmx4O"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}